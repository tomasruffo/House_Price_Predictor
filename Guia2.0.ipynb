{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contenido teorico avanzado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTERVIEW TEORICAL QUESTIONS: https://www.springboard.com/blog/machine-learning-interview-questions/\n",
    "* Transformaciones de fourier: https://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/\n",
    "* Likelihood vs probability : https://stats.stackexchange.com/questions/2641/what-is-the-difference-between-likelihood-and-probability#2647\n",
    "* Generative and Discriminative Models: https://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-a-discriminative-algorithm\n",
    "Cómo podemos saber si nuestro modelo es bueno?\n",
    "Teóricamente:\n",
    "Campo interesante: teoría de la generalización\n",
    "Según las ideas de medir la simplicidad/complejidad del modelo\n",
    "Intuición: formalización del principio de la navaja de Ockham\n",
    "Cuanto menos complejo es un modelo, más probable es que un buen resultado empírico no se deba simplemente a las peculiaridades de nuestra muestra\n",
    "\n",
    "Cuanto menos complejo sea un modelo de AA, más probable será que un buen resultado empírico no se deba simplemente a las peculiaridades de la muestra.\n",
    "\n",
    "\n",
    "Sugerencia\n",
    "Los conjuntos de prueba y de validación se \"desgastan\" con el uso repetido. Esto significa que cuanto más usas los mismos datos para tomar decisiones sobre la configuración de hiperparámetros u otras mejoras del modelo, menos seguridad tendrás de que esos resultados puedan realizar generalizaciones sobre datos nuevos nunca antes vistos. Ten en cuenta que los conjuntos de validación suelen desgastarse más lentamente que los conjuntos de prueba.\n",
    "\n",
    "Si es posible, una buena idea es recopilar más datos para \"actualizar\" el conjunto de prueba y el de validación. Comenzar desde cero es un gran restablecimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las condiciones del AA\n",
    "Las tres suposiciones básicas siguientes guían la generalización:\n",
    "\n",
    "Los ejemplos se obtienen independiente e idénticamente (i.i.d) de manera aleatoria de la distribución. En otras palabras, los ejemplos no se influyen entre sí. (Una explicación alternativa: i.i.d. es una forma de hacer referencia a la aleatoriedad de las variables).\n",
    "La distribución es estacionaria, es decir, no cambia dentro del conjunto de datos.\n",
    "Los ejemplos se obtienen de particiones de la misma distribución.\n",
    "En la práctica, a veces infringimos estas suposiciones. Por ejemplo:\n",
    "\n",
    "Considera un modelo que elige los anuncios para mostrar. La suposición de i.i.d. se infringiría si, en parte, el modelo basara su elección en función de los anuncios que el usuario visualizó anteriormente.\n",
    "Considera un conjunto de datos que contenga la información de ventas minoristas de un año. Las compras de los usuarios cambian todas las temporadas, lo cual infringiría la estacionariedad.\n",
    "Cuando sabemos que se infringe alguna de las tres suposiciones básicas anteriores, debemos prestar mucha atención a las métricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferencias entre regularizaciones L1 y L2\n",
    "L2 y L1 penalizan las ponderaciones de diferente modo:\n",
    "\n",
    "L2 penaliza ponderación2.\n",
    "L1 penaliza |ponderación|.\n",
    "En consecuencia, L2 y L1 tienen diferentes derivadas:\n",
    "\n",
    "La derivada de L2 es 2 * ponderación.\n",
    "La derivada de L1 es k (una constante cuyo valor es independiente de la ponderación).\n",
    "\n",
    "Una potencia de regularización de 0.1 debería ser suficiente. Ten en cuenta que se debe lograr un equilibrio: la regularización más potente nos da modelos más pequeños, pero puede afectar la pérdida de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SESGOS\n",
    "https://developers.google.com/machine-learning/crash-course/fairness/types-of-bias?authuser=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTACION DE MACHINE LEARNING \n",
    "https://developers.google.com/machine-learning/guides/rules-of-ml?authuser=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalización de los árboles de decisión\n",
    "Los árboles de decisión son modelos no-paramétricos. Esto significa que no sabemos cuántos parámetros vamos a tener hasta que no veamos los datos y construyamos el árbol. La regresión lineal es un modelo paramétrico porque antes de ver los datos podemos decir con toda certeza cuántos parámetros vamos a necesitar.\n",
    "\n",
    "Ventaja de los modelos no-paramétricos\n",
    "La principal ventaja de los árboles de decisión, como modelo no-paramétrico, es que pueden aprender cualquier cosa. En el siguiente gráfico vemos cómo aprende sin problemas una parábola con ruido (a la izquierda) y una función sinusoidal con ruido (a la derecha).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "data.tail()\n",
    "\n",
    "data.shape\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### loc and iloc are row-first, column-second. .[:10] loc inlcuye el 1o y iloc es hasta 9\n",
    "\n",
    "# loc obtiene filas (o columnas) con particular labels del índice.\n",
    "# iloc obtiene filas (o columnas) en determinadas posiciones en el índice (por lo que solo toma números enteros).\n",
    "# \n",
    "cols = ['country', 'variety']\n",
    "df = reviews.loc[:99, cols]\n",
    "or\n",
    "\n",
    "cols_idx = [0, 11]\n",
    "df = reviews.iloc[:100, cols_idx]\n",
    "\n",
    "#valores unicos\n",
    "reviews.taster_name.unique()\n",
    "\n",
    "\n",
    "#descripcion\n",
    "reviews.points.describe()\n",
    "\n",
    "#contador de valores\n",
    "\n",
    "reviews.taster_name.value_counts()\n",
    "\n",
    "#subdataset\n",
    "data = full_data.loc[:, [\"CLASS\", \"COUNTY\", \"geometry\"]].copy()\n",
    "\n",
    "\n",
    "# maps \n",
    "\n",
    "review_points_mean = reviews.points.mean()\n",
    "reviews.points.map(lambda p: p - review_points_mean)\n",
    "\n",
    "#contar variables\n",
    "reviews_per_country = reviews.country.value_counts()\n",
    "\n",
    "\n",
    "def remean_points(row):\n",
    "    row.points = row.points - review_points_mean\n",
    "    return row\n",
    "\n",
    "reviews.apply(remean_points, axis='columns')\n",
    "\n",
    "\n",
    "\n",
    "#I'm an economical wine buyer. Which wine is the \"best bargain\"? Create a variable bargain_wine with the title of the wine with the highest points-to-price ratio in the dataset\n",
    "bargain_idx = (reviews.points / reviews.price).idxmax()\n",
    "bargain_wine = reviews.loc[bargain_idx, 'title']\n",
    "\n",
    "\n",
    "#CONTADOR DE PALBRAS EN DSECRPCION\n",
    "\n",
    "n_trop = reviews.description.map(lambda desc: \"tropical\" in desc).sum()\n",
    "n_fruity = reviews.description.map(lambda desc: \"fruity\" in desc).sum()\n",
    "descriptor_counts = pd.Series([n_trop, n_fruity], index=['tropical', 'fruity'])\n",
    "\n",
    "# aplicar rattings de vino seguno su puntuacion(canada tiene 5 por pagar)\n",
    "\n",
    "def stars(row):\n",
    "    if row.country == 'Canada':\n",
    "        return 3\n",
    "    elif row.points >= 95:\n",
    "        return 3\n",
    "    elif row.points >= 85:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "star_ratings = reviews.apply(stars, axis='columns')\n",
    "\n",
    "\n",
    "# obtener mas barato para cada categoria \n",
    "reviews.groupby('points').price.min()\n",
    "\n",
    "#mejor obejto segun dos categorias\n",
    "\n",
    "reviews.groupby(['country', 'province']).apply(lambda df: df.loc[df.points.idxmax()])\n",
    "\n",
    "\n",
    "#analisis estadistico simple de nuestro dataset\n",
    "reviews.groupby(['country']).price.agg([len, min, max])\n",
    "\n",
    "#Ordenamiento de datos\n",
    "countries_reviewed.sort_values(by='len', ascending=False)\n",
    "\n",
    "# Ordenar con dos variables\n",
    "\n",
    "countries_reviewed.sort_values(by=['country', 'len'])\n",
    "\n",
    "# que vino es mejor por una cierta cantidad de dinero\n",
    "best_rating_per_price = reviews.groupby('price')['points'].max().sort_index()\n",
    "# minimo y maximo por variedad\n",
    "price_extremes = reviews.groupby(\"variety\").price.agg([min,max])\n",
    "\n",
    "\n",
    "# Remplazar NaN  por un valor determinado\n",
    "reviews.region_2.fillna(\"Unknown\")\n",
    "\n",
    "# Remplazar un valor por otro\n",
    "reviews.taster_twitter_handle.replace(\"@kerinokeefe\", \"@kerino\")\n",
    "\n",
    "# Re indexar fila\n",
    "\n",
    "reindexed = reviews.rename_axis('wines', axis='rows')\n",
    "\n",
    "loc = por nombre\n",
    "iloc =  por indice\n",
    "\n",
    "\n",
    "\n",
    "# Sacar valor si:\n",
    "print('Old size: %d' % len(train_df))\n",
    "train_df = train_df[(train_df.abs_diff_longitude < 5.0) & (train_df.abs_diff_latitude < 5.0)]\n",
    "print('New size: %d' % len(train_df))\n",
    "\n",
    "# Conditional selection\n",
    "data.loc[data.country == 'Italy']\n",
    "# dos condiciones\n",
    "reviews.loc[(reviews.country == 'Italy') & (reviews.points >= 90)]\n",
    "# or\n",
    "reviews.loc[(reviews.country == 'Italy') | (reviews.points >= 90)]\n",
    "#is in\n",
    "reviews.loc[reviews.country.isin(['Italy', 'France'])]\n",
    "\n",
    "#asignar data\n",
    "reviews['index_backwards'] = range(len(reviews), 0, -1)\n",
    "reviews['index_backwards']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CREAR COLUMNA CON TODO UN MISMO VALOR\n",
    "data15 = data15.assign(Fecha=\"2015/01/01\")\n",
    "\n",
    "\n",
    "#BUSCAR VALORESEN FILAS\n",
    "data[data.bidder == 'parakeet2004']['bidderrate'] = 100\n",
    "\n",
    "#LISTA DE COLUMNAS\n",
    "data.columns.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agrupar por algo y ver como se comporta segun el target\n",
    "df=train.groupby(['Gender','Response'])['id'].count().to_frame().rename(columns={'id':'count'}).reset_index()\n",
    "\n",
    "\n",
    "g = sns.catplot(x=\"Gender\", y=\"count\",col=\"Response\",\n",
    "                data=df, kind=\"bar\",\n",
    "                height=4, aspect=.7);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALORES RANDOM\n",
    "np.random.seed(19796) \n",
    "dataset['sexo'] = np.random.choice(  \n",
    "    a=['masculino', 'femenino'],  \n",
    "    size=19796,  \n",
    "     p=[0.52, 0.48]  \n",
    "                       ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agrupar por edad\n",
    "# Convertir edades en categorias\n",
    "edades = [0, 5, 12, 18, 35, 60, 100]\n",
    "categorias = [\"0-4\", \"5-11\", \"11-17\", \"18-34\", \"35-59\", \"60-100\"]\n",
    "\n",
    "cat_edad = pd.cut(data[\"Edad\"],edades, labels= categorias) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if en pandas\n",
    "f.loc[df['column name'] condition, 'new column name'] \n",
    "\n",
    "#SUMA MOROSA\n",
    "\n",
    "data.loc[data['Sit 1'] >2, 'sit1_1'] = data[\"Monto 1\"]\n",
    "data.loc[data['Sit 2'] >2, 'sit1_2'] = data[\"Monto 2\"]\n",
    "data.loc[data['Sit 3'] >2, 'sit1_3'] = data[\"Monto 3\"]\n",
    "data.loc[data['Sit 4'] >2, 'sit1_4'] = data[\"Monto 4\"]\n",
    "data.loc[data['Sit 5'] >2, 'sit1_5'] = data[\"Monto 5\"]\n",
    "data.loc[data['Sit 6'] >2, 'sit1_6'] = data[\"Monto 6\"]\n",
    "data['suma morosa'] = data[[\"sit1_1\", \"sit1_2\", \"sit1_3\", \"sit1_4\", \"sit1_5\", \"sit1_6\"]].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metodo 2 de if\n",
    "for i in range(1,6):\n",
    "    data['sit1_{}'.format(i)] = data['Sit {}'.format(i)].apply(lambda x: data[\"Monto {}\".format(i)] if x > 2 else 0 )\n",
    "data['cantidad_creditos_morosos'] = data[[\"sit1_1\", \"sit1_2\", \"sit1_3\", \"sit1_4\", \"sit1_5\", \"sit1_6\"]].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEPARAR COLUMNAS (DIVIDIR CONTENDIO DE UNA MISMA COLUMNA)\n",
    "\n",
    "#separo la columna precio\n",
    "data[[\"fecha\",\"hora\"]]=  data[\"fecha\"].str.split(pat= \"|\",expand=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAMBIAR NOMBRE DE CULUMNAS\n",
    "data13 = data13.rename(columns={'LONGITUD':'longitud',\n",
    "                                   'LATITUD':'latitud',\n",
    "                                   'FECHA':'fecha',\n",
    "                                   'CALLE':'calle',\n",
    "                                   'NUMERO':'numero',\n",
    "                                   'M2':'m2',\n",
    "                                   'DOLARES':'dolares',\n",
    "                                   'U_S_M2':'u_s_m2',\n",
    "                                   'Z':'pesos',\n",
    "                                   'Z_M2':'m2__pesos_',\n",
    "                                   'LAT':'latitud',\n",
    "                                   'BARRIOS':'barrio',\n",
    "                                   'COMUNA':'comuna'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORRELACION DE VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_dataframe = training_examples.copy()\n",
    "correlation_dataframe[\"target\"] = training_targets[\"median_house_value\"]\n",
    "\n",
    "correlation_dataframe.corr()\n",
    "\n",
    "\n",
    "corr.style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST DE CHI CUADRADO\n",
    "df = df[df[\"Origin\"].isin([\"HOU\",\"ATL\",\"IND\"])]\n",
    "df = df.sample(frac=1)\n",
    "df = df[0:10000]\n",
    "\n",
    "df[\"Bigdelay\"] =  df[\"ArrDelay\"] >30\n",
    "observados = pd.crosstab(index=df[\"Bigdelay\"],columns=df[\"Origin\"],margin=True)\n",
    "\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "test = chi2_contingency(observados)\n",
    "test\n",
    "esperados = pd.DataFrame(test[3])\n",
    "esperados_rel= round(esperados.apply(lambda r : r/len(df)*100,axis=1),2)\n",
    "observados_rel= round(observados.apply(lambda r : r/len(df)*100,axis=1),2)\n",
    "\n",
    "test[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRAFICOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AGRUPAR Y VER CUAL CUAL CUMPLE CON UNA DETERMINADA CARACTERISTICA\n",
    "\n",
    "class_df = df.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\n",
    "class_df.style.background_gradient(cmap='winter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REPRESENTARLO EN UN GRAFICO\n",
    "percent_class=class_df.text\n",
    "labels= class_df.sentiment\n",
    "\n",
    "colors = ['#17C37B','#F92969','#FACA0C']\n",
    "\n",
    "my_pie,_,_ = plt.pie(percent_class,radius = 1.2,labels=labels,colors=colors,autopct=\"%.1f%%\")\n",
    "\n",
    "plt.setp(my_pie, width=0.6, edgecolor='white') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar dos clases\n",
    "cat_cols = [\"telecommuting\", \"has_company_logo\", \"has_questions\", \"employment_type\", \"required_experience\", \"required_education\",]\n",
    "# visualizating catagorical variable by target\n",
    "import matplotlib.gridspec as gridspec # to do the grid of plots\n",
    "grid = gridspec.GridSpec(3, 3, wspace=0.5, hspace=0.5) # The grid of chart\n",
    "plt.figure(figsize=(15,25)) # size of figure\n",
    "\n",
    "# loop to get column and the count of plots\n",
    "for n, col in enumerate(cat_df[cat_cols]): \n",
    "    ax = plt.subplot(grid[n]) # feeding the figure of grid\n",
    "    sns.countplot(x=col, data=cat_df, hue='fraudulent', palette='Set2') \n",
    "    ax.set_ylabel('Count', fontsize=12) # y axis label\n",
    "    ax.set_title(f'{col} Distribution by Target', fontsize=15) # title label\n",
    "    ax.set_xlabel(f'{col} values', fontsize=12) # x axis label\n",
    "    xlabels = ax.get_xticklabels() \n",
    "    ylabels = ax.get_yticklabels() \n",
    "    ax.set_xticklabels(xlabels,  fontsize=10)\n",
    "    ax.set_yticklabels(ylabels,  fontsize=10)\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.xticks(rotation=90) \n",
    "    total = len(cat_df)\n",
    "    sizes=[] # Get highest values in y\n",
    "    for p in ax.patches: # loop to all objects\n",
    "        height = p.get_height()\n",
    "        sizes.append(height)\n",
    "        ax.text(p.get_x()+p.get_width()/2.,\n",
    "                height + 3,\n",
    "                '{:1.2f}%'.format(height/total*100),\n",
    "                ha=\"center\", fontsize=10) \n",
    "    ax.set_ylim(0, max(sizes) * 1.15) #set y limit based on highest heights\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2)\n",
    "\n",
    "traces = [\n",
    "    go.Histogram(\n",
    "        x=train['Age'], \n",
    "        name='Train Age'\n",
    "    ),\n",
    "    go.Histogram(\n",
    "        x=test['Age'], \n",
    "        name='Test Age'\n",
    "    ),\n",
    "\n",
    "]\n",
    "\n",
    "for i in range(len(traces)):\n",
    "    fig.append_trace(traces[i], (i // 2) + 1, (i % 2)  +1)\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Train/test Age column distribution',\n",
    "    height=500,\n",
    "    width=900\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\n",
    "length=text_df[text_df[\"fraudulent\"]==1]['description'].str.len()\n",
    "ax1.hist(length,bins = 20,color='orangered')\n",
    "ax1.set_title('Fake Post')\n",
    "length=text_df[text_df[\"fraudulent\"]==0]['description'].str.len()\n",
    "ax2.hist(length, bins = 20)\n",
    "ax2.set_title('Real Post')\n",
    "fig.suptitle('Characters in description')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAFICO DE LA VARIABLE PRINCIPAL\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "sns.distplot(train['SalePrice'] , fit=norm);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(train['SalePrice'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(train['SalePrice'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA WRANGLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remplazar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['Columna'] == 2207,'Columna/'] = 2007\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Columnas bucketizadas(revisar)\n",
    "https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantile_based_boundaries(feature_values, num_buckets):\n",
    "  boundaries = np.arange(1.0, num_buckets) / num_buckets\n",
    "  quantiles = feature_values.quantile(boundaries)\n",
    "  return [quantiles[q] for q in quantiles.keys()]\n",
    "\n",
    "# Divide households into 7 buckets.\n",
    "households = tf.feature_column.numeric_column(\"households\")\n",
    "bucketized_households = tf.feature_column.bucketized_column(\n",
    "  households, boundaries=get_quantile_based_boundaries(\n",
    "    california_housing_dataframe[\"households\"], 7))\n",
    "\n",
    "# Divide longitude into 10 buckets.\n",
    "longitude = tf.feature_column.numeric_column(\"longitude\")\n",
    "bucketized_longitude = tf.feature_column.bucketized_column(\n",
    "  longitude, boundaries=get_quantile_based_boundaries(\n",
    "    california_housing_dataframe[\"longitude\"], 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recomendations:\n",
    "* if the column have 60% of missing values, drop the column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Columna\"] = data['columna'].fillna(full_df[\"YearBuilt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# porcentaje de valores vacios por columna\n",
    "total = df_train.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dropear columnas con mas del 65% de los valores como  NA\n",
    "data_filter = data.loc[:, data.isnull().mean() <= .65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dropear filas con mas de dos variables como na\n",
    "\n",
    "data_filter = data.dropna(how=\"any\")\n",
    "data_filter = data.dropna(subset =[\"column1\",\"column2\"])  #una de las dos variables tiene valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the NaN values of each of the numerical columns with the median of each one\n",
    "\n",
    "for column in numerical_cols:\n",
    "    trainingData2[column].fillna(trainingData2[column].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the NaN values of each of the categorical columns with the mode of each one\n",
    "\n",
    "for column in categorical_cols:\n",
    "    trainingData2[column].fillna(trainingData2[column].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the Nan values with \"Unknow\"\n",
    "\n",
    "data[\"columna\"].fillna(value=\"Unknow\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cantidad de NaN por columna\n",
    "nullColumns = []    #Lista que contendrá las columnas con valores Nulos\n",
    "for column in data.columns:\n",
    "    if data[column].isnull().sum() > 0:\n",
    "        print(column, data[column].isnull().sum())\n",
    "        nullColumns.append(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Low and High cardinality split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = [cname for cname in pfeatures.columns if \n",
    "                pfeatures[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "categorical_cols = [cname for cname in pfeatures.columns if \n",
    "                   pfeatures[cname].dtype == \"object\"]\n",
    "\n",
    "\n",
    "\n",
    "#prueba de la cardinalidad\n",
    "#A las columnas con baja cardinalidad,se le aplicara un One-Hot-Encoder\n",
    "low_cardinality_cols = [col for col in categorical_cols if data[col].nunique() < 10]\n",
    "\n",
    "#A las columnas con alta cardinalidad, se le aplicara un label_encoder\n",
    "high_cardinality_cols = list(set(categorical_cols)-set(low_cardinality_cols))\n",
    "\n",
    "low_cardinality_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols = pd.DataFrame(OH_encoder.fit_transform(data[low_cardinality_cols]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Make copy to avoid changing original data \n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "# Apply label encoder to each column with categorical data\n",
    "label_encoder = LabelEncoder()\n",
    "for col in object_cols:\n",
    "    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n",
    "    label_X_valid[col] = label_encoder.transform(X_valid[col])\n",
    "\n",
    "print(\"MAE from Approach 2 (Label Encoding):\") \n",
    "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders.count import CountEncoder\n",
    "count_enc = ce.CountEncoder()\n",
    "\n",
    "count_encoded = count_enc.fit_transform(ks[cat_features])\n",
    "data = data.join(count_encoded.add_suffix(\"_count\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problema indices\n",
    "#los indices de esta columna comienzan en 0 y los de pfeatures comienzan en 1,\n",
    "#voy a hacer que el indice de esta columna tambien empieze en 1 para evitar problemas al unir los datasets\n",
    "OH_encoder.index = np.arange(1, len(OH_encoder)+1)\n",
    "OH_encoder.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quartiles\n",
    "\n",
    "q1=np.percentile(x,25)\n",
    "q3=np.percentile(x,75)\n",
    "rangointer = q3 -q1\n",
    "umbralsuperior = q3 + 1.5*rangointer\n",
    "umbralinferior   q1 + 1.5*rangointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(x > umbralsuperior)\n",
    "np.mean(x< umbralinferior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo que selecciona \n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "outliers =EllipticEnvelope(contamination=.01)\n",
    "var_list= [\"columna1\"]\n",
    "\n",
    "x= np.array(df.loc[:var_list].dropna())\n",
    "\n",
    "outliers.fit(x)\n",
    "\n",
    "pred = outliers.predict(x)\n",
    "pred\n",
    "elipt_outliers= np.where(pred==-1)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columnas de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASETS DESBALANCEADOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cambiar tipo de mustreo\n",
    "* submusetreo \n",
    "* sobre muestreo aleatorio( datasets no tan grandes)\n",
    "* SMOTE:\n",
    "\n",
    "\n",
    "#### en estos modelos no hay que usar la precission para medirlos, hay que utilizar\n",
    "* presisicion y especifidad\n",
    "* sensebilidad o recall\n",
    "* puntaje f1 \n",
    "* auc\n",
    "\n",
    "https://imbalanced-learn.readthedocs.io/en/stable/over_sampling.html\n",
    "\n",
    "### aca son mas utiles los modelos\n",
    "\n",
    "Check by using Different Metrices\n",
    "Accuracy metric is not best metric to use when evaluating imbalanced class as it can be mislead for the classification.\n",
    "Following are some metrics give us the good insights on the imbalanced dataset\n",
    "\n",
    "* 1. Confusion Metrics : confusion metrics show the clearly classification of the predicted class vs actual class.We can also see how many data point wrongly classified. \n",
    "* 2. Precision : We can get the precision by number of all positive classified value divided by all positive predicted value.It's measure the classifier's exactness.Low presicion indicates the high number of false positive. \n",
    "* 3. Recall : Recall is a metric that quantifies the number of correct positive predictions made out of all positive predictions that could have been made. Unlike precision that only comments on the correct positive predictions out of all positive predictions, recall provides an indication of missed positive predictions. \n",
    "* 4. F1-score : The F1-measure, which weights precision and recall equally, is the variant most often used when learning from imbalanced data. \n",
    "* 5. Classification Report : All above metioned things are auto-generated in the classication report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smote\n",
    "#import the libaray for SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Separate input features and target\n",
    "y = data[\"Class\"]\n",
    "X = data.drop('Class', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "sm = SMOTE(random_state=27,sampling_strategy=1.0)\n",
    "X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "\n",
    "smote_logistic = LogisticRegression()\n",
    "smote_logistic.fit(X_train,y_train)\n",
    "\n",
    "smote_pred = smote_logistic.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar valores unicos de una variable\n",
    "data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARAR VARIABLE OBJETIVO CON LA OTRA\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\n",
    "plt.tight_layout()\n",
    "\n",
    "df[\"fraudulent\"].value_counts().plot(kind='pie', ax=axes[0], labels=['Real Post (95%)', 'Fake Post (5%)'])\n",
    "temp = df[\"fraudulent\"].value_counts()\n",
    "sns.barplot(temp.index, temp, ax=axes[1])\n",
    "\n",
    "axes[0].set_ylabel(' ')\n",
    "axes[1].set_ylabel(' ')\n",
    "axes[1].set_xticklabels([\"Real Post (17014) [0's]\", \"Fake Post (866) [1's]\"])\n",
    "\n",
    "axes[0].set_title('Target Distribution in Dataset', fontsize=13)\n",
    "axes[1].set_title('Target Count in Dataset', fontsize=13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIMENSION REDUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESTANDARIZAR DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estadarizo los datos para que funcione mejor el modelo\n",
    "from  sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc_X = StandardScaler()\n",
    "X_train_sca = sc_X.fit_transform(X_train)\n",
    "X_test_sca = sc_X.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA SETS DESBALANCEADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Smote\n",
    "#import the libaray for SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Separate input features and target\n",
    "y = data[\"Class\"]\n",
    "X = data.drop('Class', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "sm = SMOTE(random_state=27,sampling_strategy=1.0)\n",
    "X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "\n",
    "smote_logistic = LogisticRegression()\n",
    "smote_logistic.fit(X_train,y_train)\n",
    "\n",
    "smote_pred = smote_logistic.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL APPLICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LINEAR REGRESSION WITH TENSORFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input feature: total_rooms.\n",
    "my_feature = california_housing_dataframe[[\"total_rooms\"]]\n",
    "# Define the label.\n",
    "targets = california_housing_dataframe[\"median_house_value\"]\n",
    "\n",
    "# Configure a numeric feature column for total_rooms.\n",
    "feature_columns = [tf.feature_column.numeric_column(\"total_rooms\")]\n",
    "# Use gradient descent as the optimizer for training the model.\n",
    "my_optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.0000001)\n",
    "my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "\n",
    "# Configure the linear regression model with our feature columns and optimizer.\n",
    "# Set a learning rate of 0.0000001 for Gradient Descent.\n",
    "linear_regressor = tf.estimator.LinearRegressor(\n",
    "    feature_columns=feature_columns,\n",
    "    optimizer=my_optimizer\n",
    ")\n",
    "\n",
    "def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n",
    "    \"\"\"Trains a linear regression model of one feature.\n",
    "  \n",
    "    Args:\n",
    "      features: pandas DataFrame of features\n",
    "      targets: pandas DataFrame of targets\n",
    "      batch_size: Size of batches to be passed to the model\n",
    "      shuffle: True or False. Whether to shuffle the data.\n",
    "      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n",
    "    Returns:\n",
    "      Tuple of (features, labels) for next data batch\n",
    "    \"\"\"\n",
    "  \n",
    "    # Convert pandas data into a dict of np arrays.\n",
    "    features = {key:np.array(value) for key,value in dict(features).items()}                                           \n",
    " \n",
    "    # Construct a dataset, and configure batching/repeating.\n",
    "    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n",
    "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "    \n",
    "    # Shuffle the data, if specified.\n",
    "    if shuffle:\n",
    "      ds = ds.shuffle(buffer_size=10000)\n",
    "    \n",
    "    # Return the next batch of data.\n",
    "    features, labels = ds.make_one_shot_iterator().get_next()\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "_ = linear_regressor.train(\n",
    "    input_fn = lambda:my_input_fn(my_feature, targets),\n",
    "    steps=100\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Create an input function for predictions.\n",
    "# Note: Since we're making just one prediction for each example, we don't \n",
    "# need to repeat or shuffle the data here.\n",
    "prediction_input_fn =lambda: my_input_fn(my_feature, targets, num_epochs=1, shuffle=False)\n",
    "\n",
    "# Call predict() on the linear_regressor to make predictions.\n",
    "predictions = linear_regressor.predict(input_fn=prediction_input_fn)\n",
    "\n",
    "# Format predictions as a NumPy array, so we can calculate error metrics.\n",
    "predictions = np.array([item['predictions'][0] for item in predictions])\n",
    "\n",
    "# Print Mean Squared Error and Root Mean Squared Error.\n",
    "mean_squared_error = metrics.mean_squared_error(predictions, targets)\n",
    "root_mean_squared_error = math.sqrt(mean_squared_error)\n",
    "print(\"Mean Squared Error (on training data): %0.3f\" % mean_squared_error)\n",
    "print(\"Root Mean Squared Error (on training data): %0.3f\" % root_mean_squared_error)\n",
    "\n",
    "\n",
    "\n",
    "min_house_value = california_housing_dataframe[\"median_house_value\"].min()\n",
    "max_house_value = california_housing_dataframe[\"median_house_value\"].max()\n",
    "min_max_difference = max_house_value - min_house_value\n",
    "\n",
    "print(\"Min. Median House Value: %0.3f\" % min_house_value)\n",
    "print(\"Max. Median House Value: %0.3f\" % max_house_value)\n",
    "print(\"Difference between Min. and Max.: %0.3f\" % min_max_difference)\n",
    "print(\"Root Mean Squared Error: %0.3f\" % root_mean_squared_error)\n",
    "\n",
    "\n",
    "sample = california_housing_dataframe.sample(n=300)\n",
    "\n",
    "\n",
    "# Get the min and max total_rooms values.\n",
    "x_0 = sample[\"total_rooms\"].min()\n",
    "x_1 = sample[\"total_rooms\"].max()\n",
    "\n",
    "# Retrieve the final weight and bias generated during training.\n",
    "weight = linear_regressor.get_variable_value('linear/linear_model/total_rooms/weights')[0]\n",
    "bias = linear_regressor.get_variable_value('linear/linear_model/bias_weights')\n",
    "\n",
    "# Get the predicted median_house_values for the min and max total_rooms values.\n",
    "y_0 = weight * x_0 + bias \n",
    "y_1 = weight * x_1 + bias\n",
    "\n",
    "# Plot our regression line from (x_0, y_0) to (x_1, y_1).\n",
    "plt.plot([x_0, x_1], [y_0, y_1], c='r')\n",
    "\n",
    "# Label the graph axes.\n",
    "plt.ylabel(\"median_house_value\")\n",
    "plt.xlabel(\"total_rooms\")\n",
    "\n",
    "# Plot a scatter plot from our data sample.\n",
    "plt.scatter(sample[\"total_rooms\"], sample[\"median_house_value\"])\n",
    "\n",
    "# Display graph.\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def train_model(learning_rate, steps, batch_size, input_feature=\"total_rooms\"):\n",
    "  \"\"\"Trains a linear regression model of one feature.\n",
    "  \n",
    "  Args:\n",
    "    learning_rate: A `float`, the learning rate.\n",
    "    steps: A non-zero `int`, the total number of training steps. A training step\n",
    "      consists of a forward and backward pass using a single batch.\n",
    "    batch_size: A non-zero `int`, the batch size.\n",
    "    input_feature: A `string` specifying a column from `california_housing_dataframe`\n",
    "      to use as input feature.\n",
    "  \"\"\"\n",
    "  \n",
    "  periods = 10\n",
    "  steps_per_period = steps / periods\n",
    "\n",
    "  my_feature = input_feature\n",
    "  my_feature_data = california_housing_dataframe[[my_feature]]\n",
    "  my_label = \"median_house_value\"\n",
    "  targets = california_housing_dataframe[my_label]\n",
    "\n",
    "  # Create feature columns.\n",
    "  feature_columns = [tf.feature_column.numeric_column(my_feature)]\n",
    "  \n",
    "  # Create input functions.\n",
    "  training_input_fn = lambda:my_input_fn(my_feature_data, targets, batch_size=batch_size)\n",
    "  prediction_input_fn = lambda: my_input_fn(my_feature_data, targets, num_epochs=1, shuffle=False)\n",
    "  \n",
    "  # Create a linear regressor object.\n",
    "  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "  linear_regressor = tf.estimator.LinearRegressor(\n",
    "      feature_columns=feature_columns,\n",
    "      optimizer=my_optimizer\n",
    "  )\n",
    "\n",
    "  # Set up to plot the state of our model's line each period.\n",
    "  plt.figure(figsize=(15, 6))\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.title(\"Learned Line by Period\")\n",
    "  plt.ylabel(my_label)\n",
    "  plt.xlabel(my_feature)\n",
    "  sample = california_housing_dataframe.sample(n=300)\n",
    "  plt.scatter(sample[my_feature], sample[my_label])\n",
    "  colors = [cm.coolwarm(x) for x in np.linspace(-1, 1, periods)]\n",
    "\n",
    "  # Train the model, but do so inside a loop so that we can periodically assess\n",
    "  # loss metrics.\n",
    "  print(\"Training model...\")\n",
    "  print(\"RMSE (on training data):\")\n",
    "  root_mean_squared_errors = []\n",
    "  for period in range (0, periods):\n",
    "    # Train the model, starting from the prior state.\n",
    "    linear_regressor.train(\n",
    "        input_fn=training_input_fn,\n",
    "        steps=steps_per_period\n",
    "    )\n",
    "    # Take a break and compute predictions.\n",
    "    predictions = linear_regressor.predict(input_fn=prediction_input_fn)\n",
    "    predictions = np.array([item['predictions'][0] for item in predictions])\n",
    "    \n",
    "    # Compute loss.\n",
    "    root_mean_squared_error = math.sqrt(\n",
    "        metrics.mean_squared_error(predictions, targets))\n",
    "    # Occasionally print the current loss.\n",
    "    print(\"  period %02d : %0.2f\" % (period, root_mean_squared_error))\n",
    "    # Add the loss metrics from this period to our list.\n",
    "    root_mean_squared_errors.append(root_mean_squared_error)\n",
    "    # Finally, track the weights and biases over time.\n",
    "    # Apply some math to ensure that the data and line are plotted neatly.\n",
    "    y_extents = np.array([0, sample[my_label].max()])\n",
    "    \n",
    "    weight = linear_regressor.get_variable_value('linear/linear_model/%s/weights' % input_feature)[0]\n",
    "    bias = linear_regressor.get_variable_value('linear/linear_model/bias_weights')\n",
    "\n",
    "    x_extents = (y_extents - bias) / weight\n",
    "    x_extents = np.maximum(np.minimum(x_extents,\n",
    "                                      sample[my_feature].max()),\n",
    "                           sample[my_feature].min())\n",
    "    y_extents = weight * x_extents + bias\n",
    "    plt.plot(x_extents, y_extents, color=colors[period]) \n",
    "  print(\"Model training finished.\")\n",
    "\n",
    "  # Output a graph of loss metrics over periods.\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.ylabel('RMSE')\n",
    "  plt.xlabel('Periods')\n",
    "  plt.title(\"Root Mean Squared Error vs. Periods\")\n",
    "  plt.tight_layout()\n",
    "  plt.plot(root_mean_squared_errors)\n",
    "\n",
    "  # Output a table with calibration data.\n",
    "  calibration_data = pd.DataFrame()\n",
    "  calibration_data[\"predictions\"] = pd.Series(predictions)\n",
    "  calibration_data[\"targets\"] = pd.Series(targets)\n",
    "  display.display(calibration_data.describe())\n",
    "\n",
    "  print(\"Final RMSE (on training data): %0.2f\" % root_mean_squared_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)\n",
    "model_xgb.fit(X_train, y_train)\n",
    "r2_score(model_xgb.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "xgbc2 = XGBClassifier(n_estimators=1000, learning_rate=0.40)\n",
    "\n",
    "\n",
    "xgbc2.fit(X_train, y_train,\n",
    "          early_stopping_rounds  = 10 ,\n",
    "          eval_set=[(X_test, y_test)], \n",
    "             verbose=False\n",
    "         )\n",
    "\n",
    "\n",
    "xgbc2_pred =xgbc2.predict(X_test)\n",
    "print(classification_report(y_test,xgbc2_pred))\n",
    "confusion_matrix(y_test, xgbc2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_model_l = XGBClassifier(random_state = SEED, max_depth = 8, \n",
    "                            n_estimators = 30000, \n",
    "                            reg_lambda = 1.2, reg_alpha = 1.2, \n",
    "                            min_child_weight = 1, \n",
    "                            objective = 'binary:logistic',\n",
    "                            learning_rate = 0.15, gamma = 0.3, colsample_bytree = 0.5, eval_metric = 'auc')\n",
    "\n",
    "XGB_model_l.fit(X_train, y_train,\n",
    "                eval_set = [(X_valid, y_valid)],\n",
    "                early_stopping_rounds=50,verbose = 1000)\n",
    "\n",
    "\n",
    "\n",
    "XGB_preds_l = XGB_model_l.predict_proba(X_valid)\n",
    "XGB_score_l = roc_auc_score(y_valid, XGB_preds_l[:,1])\n",
    "XGB_class_l = XGB_model_l.predict(X_valid)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "xgboost_pred =xgboost.predict(X_test)\n",
    "print(classification_report(y_test,xgboost_pred))\n",
    "confusion_matrix(y_test, xgboost_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nc = range(1, 20)\n",
    "kmeans = [KMeans(n_clusters=i) for i in Nc]\n",
    "kmeans\n",
    "score = [kmeans[i].fit(X).score(X) for i in range(len(kmeans))]\n",
    "score\n",
    "plt.plot(Nc,score)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Elbow Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "cluster = 4\n",
    "\n",
    "kmeans = KMeans(n_clusters=cluster, # numero de clusters\n",
    "                init=\"random\", \n",
    "                random_state=10)\n",
    "\n",
    "kmeans.fit(X.values) # entrenamos\n",
    "\n",
    "kmeans.fit_predict(X.values).shape\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, # numero de clusters\n",
    "                init=\"random\", \n",
    "                random_state=10)\n",
    "\n",
    "X['clusters'] = kmeans.fit_predict(X.values) # entrenamos y predecimos\n",
    "\n",
    "data.head(23000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = pd.DataFrame(kmeans.cluster_centers_, columns=dataset.columns)\n",
    "centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_steps_with_tensor_flow.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEAN SQUARE ERROR AND ROOT MEAN\n",
    " Print Mean Squared Error and Root Mean Squared Error.\n",
    "mean_squared_error = metrics.mean_squared_error(predictions, targets)\n",
    "root_mean_squared_error = math.sqrt(mean_squared_error)\n",
    "print(\"Mean Squared Error (on training data): %0.3f\" % mean_squared_error)\n",
    "print(\"Root Mean Squared Error (on training data): %0.3f\" % root_mean_squared_error)\n",
    "\n",
    "\n",
    "min_house_value = california_housing_dataframe[\"median_house_value\"].min()\n",
    "max_house_value = california_housing_dataframe[\"median_house_value\"].max()\n",
    "min_max_difference = max_house_value - min_house_value\n",
    "\n",
    "print(\"Min. Median House Value: %0.3f\" % min_house_value)\n",
    "print(\"Max. Median House Value: %0.3f\" % max_house_value)\n",
    "print(\"Difference between Min. and Max.: %0.3f\" % min_max_difference)\n",
    "print(\"Root Mean Squared Error: %0.3f\" % root_mean_squared_error)\n",
    "\n",
    "\n",
    "\n",
    "calibration_data = pd.DataFrame()\n",
    "calibration_data[\"predictions\"] = pd.Series(predictions)\n",
    "calibration_data[\"targets\"] = pd.Series(targets)\n",
    "calibration_data.describe()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curva roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = model.predict_proba(x_test)[:,1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "\n",
    "title('Random Forest ROC curve: CC Fraud')\n",
    "xlabel('FPR (Precision)')\n",
    "ylabel('TPR (Recall)')\n",
    "\n",
    "plot(fpr,tpr)\n",
    "plot((0,1), ls='dashed',color='black')\n",
    "plt.show()\n",
    "print ('Area under curve (AUC): ', auc(fpr,tpr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "# ... load data\n",
    "\n",
    "perm = PermutationImportance(classifier).fit(X_test_m1, y_test_m1)\n",
    "eli5.show_weights(perm, feature_names = val_x.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEORIA HIPERPARAMETROS\n",
    " ### ¿Hay una heurística estándar para el ajuste del modelo?\n",
    "\n",
    "Esta es una pregunta frecuente. La respuesta breve es que los efectos de los diferentes hiperparámetros dependen de los datos. Por lo tanto, no hay reglas estrictas; debes probarlos en tus datos.\n",
    "\n",
    "Dicho esto, aquí se incluyen algunas reglas generales que pueden ayudarte como guía:\n",
    "\n",
    " * El error de entrenamiento debe disminuir constantemente, de manera abrupta al principio y eventualmente estancarse a medida que converge el entrenamiento.\n",
    " * Si el entrenamiento no convirgió, prueba ejecutarlo durante más tiempo.\n",
    " * Si el error de entrenamiento disminuye muy lentamente, aumentar la tasa de entrenamiento puede ayudar a que disminuya más rápido.\n",
    "   * Sin embargo, en algunas ocasiones puede ocurrir exactamente lo opuesto si la tasa de aprendizaje es demasiado alta.\n",
    " * Si el error de entrenamiento varía extremadamente, prueba disminuir la tasa de aprendizaje.\n",
    "   * Una tasa de aprendizaje más baja con un número más alto de pasos o un tamaño del lote más grande suelen ser una buena combinación.\n",
    " * Los tamaños del lote muy pequeños también pueden causar inestabilidad. Primero prueba valores más altos, como 100 o 1,000, y disminúyelos hasta que observes degradación.\n",
    "\n",
    "Como dijimos antes, nunca te rijas estrictamente por estas reglas generales, porque los efectos dependen de los datos. Siempre debes experimentar y verificar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repeticion de palabras\n",
    "\n",
    "https://www.diegocalvo.es/analisis-de-discursos-con-wordcloud-en-python/\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "#from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#elimino todas las palabras que sean conectores// remove all the words that are connectors\n",
    "stop_words_sp = set(stopwords.words('spanish'))\n",
    "\n",
    "\n",
    "\n",
    "def generateWordCloud(data,title):\n",
    "    wordcloud = WordCloud(background_color='white',\n",
    "        stopwords=stop_words_sp,\n",
    "        max_words=20,\n",
    "        max_font_size=200, \n",
    "        scale=3,\n",
    "        random_state=3).generate(str(data))\n",
    "    wordcloud.recolor(random_state=1)\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "for cols in data.index:\n",
    "  generateWordCloud(data.loc[cols,'nota'],data.loc[cols,'titulo'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONTADOR DE PALABRAS\n",
    "counter=Counter(corpus)\n",
    "most=counter.most_common()\n",
    "x=[]\n",
    "y=[]\n",
    "for word,count in most[:40]:\n",
    "    if (word not in stop) :\n",
    "        x.append(word)\n",
    "        y.append(count)\n",
    "        \n",
    "        \n",
    "sns.barplot(x=y,y=x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob: Polarity and Sensivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POLARIDAD \n",
    "* Mas cercano a -1 es NEGATIVO\n",
    "* Mas cercano a 1 es POSITIVO\n",
    "\n",
    "\n",
    "### SUBJETIVIDAD\n",
    "* Mas cercano a 0 es OBJETIVO\n",
    "* Mas cercano a 1 es SUBJETIVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evauo la polaridad con TexBlob\n",
    "\n",
    "\n",
    "eldestape[\"polaridad\"]=  eldestape[\"nota\"].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "print(\"Maximo valor\", data[\"polaridad\"].mean())\n",
    "\n",
    "\n",
    "#comparo la distribucion de las polaridades de los ultios 30 articulos\n",
    "CantidadArticulos = 30\n",
    "\n",
    "\n",
    "ax = sns.distplot(eldestape[\"polaridad\"][:CantidadArticulos],color=\"orange\",label=\"El Destape\")\n",
    "ax = sns.distplot(lanacion[\"polaridad\"][:CantidadArticulos],color=\"blue\",label='La Nacion')\n",
    "ax.set_xlabel('Polaridad',fontsize=10)\n",
    "ax.set_ylabel('Frecuencia',fontsize=10)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eldestape[\"subjetividad\"]=  eldestape[\"nota\"].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "\n",
    "#comparo la distribucion de las polaridades de los ultios 30 articulos\n",
    "CantidadArticulos = 30\n",
    "\n",
    "\n",
    "ax = sns.distplot(eldestape[\"subjetividad\"][:CantidadArticulos],color=\"orange\",label=\"El Destape\")\n",
    "ax = sns.distplot(lanacion[\"subjetividad\"][:CantidadArticulos],color=\"blue\",label='La Nacion')\n",
    "ax.set_xlabel('subjetividad',fontsize=10)\n",
    "ax.set_ylabel('Frecuencia',fontsize=10)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analisis de autores:\n",
    "https://www.kaggle.com/youhanlee/medium-author-analysis-using-spacy#Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWITTER \n",
    "https://www.kaggle.com/datatattle/covid-19-tweets-eda-viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscador de hastags\n",
    "def find_hash(text):\n",
    "    line=re.findall(r'(?<=#)\\w+',text)\n",
    "    return \" \".join(line)\n",
    "df['hash']=df['text'].apply(lambda x:find_hash(x))\n",
    "temp=df['hash'].value_counts()[:][1:11]\n",
    "temp= temp.to_frame().reset_index().rename(columns={'index':'Hashtag','hash':'count'})\n",
    "sns.barplot(x=\"Hashtag\",y=\"count\", data = temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Buscador de menciones\n",
    "def mentions(text):\n",
    "    line=re.findall(r'(?<=@)\\w+',text)\n",
    "    return \" \".join(line)\n",
    "df['mentions']=df['text'].apply(lambda x:mentions(x))\n",
    "\n",
    "temp=df['mentions'].value_counts()[:][1:11]\n",
    "temp =temp.to_frame().reset_index().rename(columns={'index':'Mentions','mentions':'count'})\n",
    "\n",
    "sns.barplot(x=\"Mentions\",y=\"count\", data = temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tri-grams\n",
    "# Trigrams\n",
    "positive_trigrams = defaultdict(int)\n",
    "neutral_trigrams = defaultdict(int)\n",
    "negative_trigrams = defaultdict(int)\n",
    "\n",
    "for tweet in train[positive]['text']:\n",
    "    for word in generate_ngrams(tweet, n_gram=3):\n",
    "        positive_trigrams[word] += 1\n",
    "        \n",
    "for tweet in train[negative]['text']:\n",
    "    for word in generate_ngrams(tweet, n_gram=3):\n",
    "        negative_trigrams[word] += 1\n",
    "        \n",
    "for tweet in train[neutral]['text']:\n",
    "    for word in generate_ngrams(tweet, n_gram=3):\n",
    "        neutral_trigrams[word] += 1        \n",
    "        \n",
    "df_positive_trigrams = pd.DataFrame(sorted(positive_trigrams.items(), key=lambda x: x[1])[::-1])\n",
    "df_negative_trigrams = pd.DataFrame(sorted(negative_trigrams.items(), key=lambda x: x[1])[::-1])\n",
    "df_neutral_trigrams = pd.DataFrame(sorted(neutral_trigrams.items(), key=lambda x: x[1])[::-1])\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(27, 30), dpi=100)\n",
    "plt.tight_layout()\n",
    "\n",
    "sns.barplot(y=df_positive_trigrams[0].values[:N], x=df_positive_trigrams[1].values[:N], ax=axes[0], color='#17C37B')\n",
    "sns.barplot(y=df_negative_trigrams[0].values[:N], x=df_negative_trigrams[1].values[:N], ax=axes[1], color='#F92969')\n",
    "sns.barplot(y=df_neutral_trigrams[0].values[:N], x=df_neutral_trigrams[1].values[:N], ax=axes[2], color='#FACA0C')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    axes[i].spines['right'].set_visible(False)\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('')\n",
    "    axes[i].tick_params(axis='x', labelsize=13)\n",
    "    axes[i].tick_params(axis='y', labelsize=13)\n",
    "\n",
    "axes[0].set_title(f'Top {N} most common trigrams in Postive Tweets', fontsize=15)\n",
    "axes[1].set_title(f'Top {N} most common trigrams in Negative Tweets', fontsize=15)\n",
    "axes[2].set_title(f'Top {N} most common trigrams in Neutral Tweets', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams\n",
    "positive_bigrams = defaultdict(int)\n",
    "neutral_bigrams = defaultdict(int)\n",
    "negative_bigrams = defaultdict(int)\n",
    "\n",
    "for tweet in train[positive]['text']:\n",
    "    for word in generate_ngrams(tweet, n_gram=2):\n",
    "        positive_bigrams[word] += 1\n",
    "        \n",
    "for tweet in train[negative]['text']:\n",
    "    for word in generate_ngrams(tweet, n_gram=2):\n",
    "        negative_bigrams[word] += 1\n",
    "        \n",
    "for tweet in train[neutral]['text']:\n",
    "    for word in generate_ngrams(tweet, n_gram=2):\n",
    "        neutral_bigrams[word] += 1        \n",
    "        \n",
    "df_positive_bigrams = pd.DataFrame(sorted(positive_bigrams.items(), key=lambda x: x[1])[::-1])\n",
    "df_negative_bigrams = pd.DataFrame(sorted(negative_bigrams.items(), key=lambda x: x[1])[::-1])\n",
    "df_neutral_bigrams = pd.DataFrame(sorted(neutral_bigrams.items(), key=lambda x: x[1])[::-1])\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(27, 30), dpi=100)\n",
    "plt.tight_layout()\n",
    "\n",
    "sns.barplot(y=df_positive_bigrams[0].values[:N], x=df_positive_bigrams[1].values[:N], ax=axes[0], color='#17C37B')\n",
    "sns.barplot(y=df_negative_bigrams[0].values[:N], x=df_negative_bigrams[1].values[:N], ax=axes[1], color='#F92969')\n",
    "sns.barplot(y=df_neutral_bigrams[0].values[:N], x=df_neutral_bigrams[1].values[:N], ax=axes[2], color='#FACA0C')\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    axes[i].spines['right'].set_visible(False)\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('')\n",
    "    axes[i].tick_params(axis='x', labelsize=13)\n",
    "    axes[i].tick_params(axis='y', labelsize=13)\n",
    "\n",
    "axes[0].set_title(f'Top {N} most common bigrams in Postive Tweets', fontsize=15)\n",
    "axes[1].set_title(f'Top {N} most common bigrams in Negative Tweets', fontsize=15)\n",
    "axes[2].set_title(f'Top {N} most common bigrams in Neutral Tweets', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PICKLE SERIALIZAR MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
